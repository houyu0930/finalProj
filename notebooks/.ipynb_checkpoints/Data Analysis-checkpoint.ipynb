{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID Tweet anger anticipation disgust fear joy love optimism pessimism sadness surprise trust\n",
    "# OOV word embedding\n",
    "\n",
    "train_path = '/home/houyu/learning/data/2018-E-c-En-train.txt'\n",
    "dev_path = '/home/houyu/learning/data/2018-E-c-En-dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-00866</td>\n",
       "      <td>@RanaAyyub @rajnathsingh Oh, hidden revenge an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-02590</td>\n",
       "      <td>I'm doing all this to make sure you smiling do...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-En-03361</td>\n",
       "      <td>if not then #teamchristine bc all tana has don...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-En-03230</td>\n",
       "      <td>It is a #great start for #beginners to jump in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-01143</td>\n",
       "      <td>My best friends driving for the first time wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-En-04301</td>\n",
       "      <td>Hey @SuperValuIRL #Fields in #skibbereen give ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-En-02651</td>\n",
       "      <td>Why have #Emmerdale had to rob #robron of havi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-En-03058</td>\n",
       "      <td>@ThomasEWoods I would like to hear a podcast o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-En-03951</td>\n",
       "      <td>If I have to hear one more time how I am intim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-En-00968</td>\n",
       "      <td>depression sucks😔</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  anger  \\\n",
       "0  2018-En-00866  @RanaAyyub @rajnathsingh Oh, hidden revenge an...      1   \n",
       "1  2018-En-02590  I'm doing all this to make sure you smiling do...      0   \n",
       "2  2018-En-03361  if not then #teamchristine bc all tana has don...      1   \n",
       "3  2018-En-03230  It is a #great start for #beginners to jump in...      0   \n",
       "4  2018-En-01143  My best friends driving for the first time wit...      0   \n",
       "5  2018-En-04301  Hey @SuperValuIRL #Fields in #skibbereen give ...      1   \n",
       "6  2018-En-02651  Why have #Emmerdale had to rob #robron of havi...      1   \n",
       "7  2018-En-03058  @ThomasEWoods I would like to hear a podcast o...      1   \n",
       "8  2018-En-03951  If I have to hear one more time how I am intim...      1   \n",
       "9  2018-En-00968                                  depression sucks😔      0   \n",
       "\n",
       "   anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
       "0             0        1     0    0     0         0          0        0   \n",
       "1             0        0     0    1     1         1          0        0   \n",
       "2             0        1     0    0     0         0          0        0   \n",
       "3             0        0     0    1     0         1          0        0   \n",
       "4             0        0     1    0     0         0          0        0   \n",
       "5             0        1     0    0     0         0          0        0   \n",
       "6             0        1     0    0     0         0          0        0   \n",
       "7             0        1     0    0     0         0          0        0   \n",
       "8             0        1     0    0     0         0          0        0   \n",
       "9             0        1     0    0     0         0          1        1   \n",
       "\n",
       "   surprise  trust  \n",
       "0         0      0  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         0      0  \n",
       "5         0      0  \n",
       "6         0      0  \n",
       "7         0      0  \n",
       "8         0      0  \n",
       "9         0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_data = pd.read_csv(\"dataset/twitter-train-B.txt\", header=None, delimiter=\"\\t\",usecols=(2,3), names=(\"sent\",\"tweet\"))\n",
    "dev_in = pd.read_csv(dev_path, engine='python', delimiter=\"\\t\")\n",
    "dev_in.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @RanaAyyub @rajnathsingh Oh, hidden revenge an...\n",
       "1      I'm doing all this to make sure you smiling do...\n",
       "2      if not then #teamchristine bc all tana has don...\n",
       "3      It is a #great start for #beginners to jump in...\n",
       "4      My best friends driving for the first time wit...\n",
       "5      Hey @SuperValuIRL #Fields in #skibbereen give ...\n",
       "6      Why have #Emmerdale had to rob #robron of havi...\n",
       "7      @ThomasEWoods I would like to hear a podcast o...\n",
       "8      If I have to hear one more time how I am intim...\n",
       "9                                      depression sucks😔\n",
       "10     O, the melancholy Catacombs quickly wandered a...\n",
       "11     #LouiseLinton - haters gonna hate keep on bein...\n",
       "12     Perfectly #Recall #amazing #surreal #totalsola...\n",
       "13     So happy to be home from Ibiza the place is so...\n",
       "14     #RIPBiwott I think Robert oukos soul can now r...\n",
       "15                   @itejasmehta @QTalker My pleasure 😊\n",
       "16     @stuael Ano man, I'm stunned! At one point our...\n",
       "17     ...with the couple, so you decide to ignore ev...\n",
       "18     @GarfieldLineker @TimCRoberts *on his back. Ap...\n",
       "19     @Dat_Phan Too much fun being interviewed by th...\n",
       "20     @pepesgrandma @FreeBeacon Thanks for the early...\n",
       "21     Time to #impeachtrump . He is #crazy #mentally...\n",
       "22     That moment when you look back and realise you...\n",
       "23     #Trump's only #concern is personal #profit thr...\n",
       "24     Shame the cashback @mbna @AmexUK credit card c...\n",
       "25     @3lectric5heep Well that must sting doesn't it...\n",
       "26     thought I was a bad driver until I went to col...\n",
       "27     I posted a snap of my dad, and someone thought...\n",
       "28     Brighten up a gloomy day with a @SiennaXOffici...\n",
       "29                                        Head hurting 😤\n",
       "                             ...                        \n",
       "856    ... or is it being locked in a loop of a daily...\n",
       "857    I finally left the dark room I've been crying ...\n",
       "858    @wvgmltd @DalefootCompost @ThomasEtty Would yo...\n",
       "859    @BecOfTheKop Something for me to ignore then. ...\n",
       "860    Some 'friends' get bitter when it seems your l...\n",
       "861    Life is too short to be jealous, hating, keepi...\n",
       "862                @uzalu_ @Veeh_Ro What a joyless cunt.\n",
       "863    @SteveBakerHW When it comes to CREDIBLE scare ...\n",
       "864    Literally kicking myself for not catching up o...\n",
       "865    @Steadi_Lady You're eating skin that could hav...\n",
       "866    Very important thing for today: \\n\\nDo not #bu...\n",
       "867    Waking up to the #scent of #coffee, there's no...\n",
       "868    Desmond was asleep when I called after I got o...\n",
       "869    @ThomasSanders This was great, looking forward...\n",
       "870    @CNN @NewDay If #trump #whitehouse aren't held...\n",
       "871    @ProfessorF @Mediaite @law_newz Childish tempe...\n",
       "872    @kamaalrkhan Which #chutiya #producer #investe...\n",
       "873    Russia story will infuriate Trump today. Media...\n",
       "874    Still 19 days left before I go home to the Phi...\n",
       "875    @AleOfATime It would be shocking, but it's sim...\n",
       "876                          Shit getting me irritated 😠\n",
       "877    Never regret anything that once made you smile...\n",
       "878    Literally hanging on by a thread need some tay...\n",
       "879    was one moron  driving his oversize tonka truc...\n",
       "880    Being deeply loved by someone gives you streng...\n",
       "881    @BadHombreNPS @SecretaryPerry If this didn't m...\n",
       "882    Excited to watch #stateoforigin tonight! Come ...\n",
       "883    Blah blah blah Kyrie, IT, etc. @CJC9BOSS leavi...\n",
       "884    #ThingsIveLearned The wise #shepherd never tru...\n",
       "885    I am really flattered and happy to hear those ...\n",
       "Name: Tweet, Length: 886, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_in['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-31436</td>\n",
       "      <td>Accept the challenges so that you can literall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-22195</td>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-En-22190</td>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-En-20221</td>\n",
       "      <td>Do you think humans have the sense for recogni...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-En-22180</td>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-En-41344</td>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-En-20759</td>\n",
       "      <td>@BossUpJaee but your pussy was weak from what ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  anger  \\\n",
       "0  2017-En-21441  “Worry is a down payment on a problem you may ...      0   \n",
       "1  2017-En-31535  Whatever you decide to do make sure it makes y...      0   \n",
       "2  2017-En-21068  @Max_Kellerman  it also helps that the majorit...      1   \n",
       "3  2017-En-31436  Accept the challenges so that you can literall...      0   \n",
       "4  2017-En-22195  My roommate: it's okay that we can't spell bec...      1   \n",
       "5  2017-En-22190  No but that's so cute. Atsu was probably shy a...      0   \n",
       "6  2017-En-20221  Do you think humans have the sense for recogni...      0   \n",
       "7  2017-En-22180  Rooneys fucking untouchable isn't he? Been fuc...      1   \n",
       "8  2017-En-41344  it's pretty depressing when u hit pan on ur fa...      0   \n",
       "9  2017-En-20759  @BossUpJaee but your pussy was weak from what ...      1   \n",
       "\n",
       "   anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
       "0             1        0     0    0     0         1          0        0   \n",
       "1             0        0     0    1     1         1          0        0   \n",
       "2             0        1     0    1     0         1          0        0   \n",
       "3             0        0     0    1     0         1          0        0   \n",
       "4             0        1     0    0     0         0          0        0   \n",
       "5             0        0     0    1     0         0          0        0   \n",
       "6             1        0     0    0     0         0          1        0   \n",
       "7             0        1     0    0     0         0          0        0   \n",
       "8             0        1     0    0     0         0          0        1   \n",
       "9             0        1     0    0     0         0          0        0   \n",
       "\n",
       "   surprise  trust  \n",
       "0         0      1  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         0      0  \n",
       "5         0      0  \n",
       "6         0      0  \n",
       "7         0      0  \n",
       "8         0      0  \n",
       "9         0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in = pd.read_csv(train_path, engine='python', delimiter=\"\\t\")\n",
    "train_in.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@', 'JJ'), ('Max_Kellerman', 'NNP'), ('it', 'PRP'), ('also', 'RB'), ('helps', 'VBZ'), ('that', 'IN'), ('the', 'DT'), ('majority', 'NN'), ('of', 'IN'), ('NFL', 'NNP'), ('coaching', 'NN'), ('is', 'VBZ'), ('inept', 'JJ'), ('.', '.'), ('Some', 'DT'), ('of', 'IN'), ('Bill', 'NNP'), (\"O'Brien\", 'NNP'), (\"'s\", 'POS'), ('play', 'NN'), ('calling', 'VBG'), ('was', 'VBD'), ('wow', 'JJ'), (',', ','), ('!', '.'), ('#', '#'), ('GOPATS', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = train_in['Tweet'][2]\n",
    "# print(text)qa\n",
    "\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "ner_list = []\n",
    "for chunk in entities:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        for _ in chunk:\n",
    "            ner_list.append(chunk.label())\n",
    "    else:\n",
    "        ner_list.append('NONE')\n",
    "# print(ner_list)\n",
    "\n",
    "ner_json = {}\n",
    "for i, ner in enumerate(ner_list):\n",
    "    ner_json[tokens[i]] = ner\n",
    "# ner_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理Tweet 分词，得到词表，将不在embedding中的词存起来，查看OOV情况\n",
    "vocab = []\n",
    "with open('vocab_train.txt', 'w') as f_out:\n",
    "    for sentence in train_in['Tweet']:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token.lower() not in vocab:\n",
    "                vocab.append(token.lower()) # 19158\n",
    "                f_out.write(token.lower()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_in.info\n",
    "# anger 2544\n",
    "# anticipation 978\n",
    "# disgust 2602\n",
    "# fear 1242\n",
    "# joy 2477\n",
    "# love 700\n",
    "# optimism 1984\n",
    "# pessimism 795\n",
    "# sadness 2008\n",
    "# surprise 361\n",
    "# trust 357\n",
    "# neutral 204\n",
    "\n",
    "cnt = 0\n",
    "for i in range(0, 6838):\n",
    "    if train_in['trust'][i]==0 and train_in['surprise'][i]==0 and train_in['sadness'][i]==0 and train_in['pessimism'][i]==0 and train_in['optimism'][i]==0 and train_in['love'][i]==0 and train_in['joy'][i]==0 and train_in['fear'][i]==0 and train_in['disgust'][i]==0 and train_in['anticipation'][i]==0 and train_in['anger'][i]==0:\n",
    "        cnt += 1\n",
    "        \n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'w') as f_out:  \n",
    "    for i in range(0, 6838):\n",
    "        f_out.write(train_in['Tweet'][i] + '\\t')\n",
    "        label = []\n",
    "        if train_in['trust'][i]==1:\n",
    "            label.append(11)\n",
    "        if train_in['surprise'][i]==1:\n",
    "            label.append(10)\n",
    "        if train_in['sadness'][i]==1:\n",
    "            label.append(9)\n",
    "        if train_in['pessimism'][i]==1:\n",
    "            label.append(8)\n",
    "        if train_in['optimism'][i]==1:\n",
    "            label.append(7)\n",
    "        if train_in['love'][i]==1:\n",
    "            label.append(6)\n",
    "        if train_in['joy'][i]==1:\n",
    "            label.append(5)\n",
    "        if train_in['fear'][i]==1:\n",
    "            label.append(4)\n",
    "        if train_in['disgust'][i]==1:\n",
    "            label.append(3)\n",
    "        if train_in['anticipation'][i]==1:\n",
    "            label.append(2)\n",
    "        if train_in['anger'][i]==1:\n",
    "            label.append(1)\n",
    "        if len(label)==0:\n",
    "            label.append(0)\n",
    "        f_out.write(str(label))\n",
    "        f_out.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.txt', 'w') as f_out:  \n",
    "    for i in range(0, 886):\n",
    "        f_out.write(train_in['Tweet'][i] + '\\t')\n",
    "        label = []\n",
    "        if train_in['trust'][i]==1:\n",
    "            label.append(11)\n",
    "        if train_in['surprise'][i]==1:\n",
    "            label.append(10)\n",
    "        if train_in['sadness'][i]==1:\n",
    "            label.append(9)\n",
    "        if train_in['pessimism'][i]==1:\n",
    "            label.append(8)\n",
    "        if train_in['optimism'][i]==1:\n",
    "            label.append(7)\n",
    "        if train_in['love'][i]==1:\n",
    "            label.append(6)\n",
    "        if train_in['joy'][i]==1:\n",
    "            label.append(5)\n",
    "        if train_in['fear'][i]==1:\n",
    "            label.append(4)\n",
    "        if train_in['disgust'][i]==1:\n",
    "            label.append(3)\n",
    "        if train_in['anticipation'][i]==1:\n",
    "            label.append(2)\n",
    "        if train_in['anger'][i]==1:\n",
    "            label.append(1)\n",
    "        if len(label)==0:\n",
    "            label.append(0)\n",
    "        f_out.write(str(label))\n",
    "        f_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-26affde6fee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdev\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdev_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdev_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_out' is not defined"
     ]
    }
   ],
   "source": [
    "# !! 标点的处理 (NLTK 拆分更彻底)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# from deepmoji.HY_tokenizer import tokenize\n",
    "\n",
    "train_in = '/home/houyu/learning/finalProj/notebooks/train.txt'\n",
    "dev_in = '/home/houyu/learning/finalProj/notebooks/dev.txt'\n",
    "train_out = '/home/houyu/learning/data/train_out.txt'\n",
    "dev_out = '/home/houyu/learning/data/dev_out.txt'\n",
    "\n",
    "def tokenize(text):\n",
    "    RE_MENTION = '@[a-zA-Z0-9_]+'\n",
    "    RE_EMOJI = \"\"\"\\ud83c[\\udf00-\\udfff]|\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|[\\u2600-\\u26FF\\u2700-\\u27BF]\"\"\"\n",
    "    intab = '“”‘’；：《》，。！？【】（）％＃＠＆１２３４５６７８９０'\n",
    "    outtab = '\"\"\\'\\';:<>,.!?[]()%#@&1234567890'\n",
    "    table= {f:t for f,t in zip(intab,outtab)}\n",
    "\n",
    "    for i in table:\n",
    "        if i in text:\n",
    "            text = text.replace(i, table[i])\n",
    "\n",
    "    # text = re.sub(RE_MENTION, \"@user\", text)\n",
    "    text = re.sub(RE_MENTION, '', text)\n",
    "    text = text.replace('\\n', '')\n",
    "    emoji_split = re.split(RE_EMOJI, text)\n",
    "    text = ' '.join(emoji_split)\n",
    "    if len(emoji_split) >= 2:\n",
    "        print(emoji_split)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove empty strings\n",
    "    result = [t.lower() for t in tokens if t.strip() is not '']\n",
    "    return result\n",
    "\n",
    "with open(dev_out, 'w') as fdev:\n",
    "    with open(dev_in, 'r') as fdev_in:\n",
    "        for line in fdev_in.readlines():\n",
    "            sp_line = line.split('\\t')\n",
    "            label = sp_line[1]\n",
    "            raw_sentence = sp_line[0]\n",
    "            sentence_list = tokenize(raw_sentence)\n",
    "            fdev.write(str(sentence_list))\n",
    "            fdev.write('\\t')\n",
    "            fdev.write(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
